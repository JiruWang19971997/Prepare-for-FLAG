# 自我介绍
您好，我于21年纽约大学毕业，专业是计算机工程，毕业后在京东工作了一年，主要负责........我比较熟悉NLP中的文本匹配，文本表征任务，以及基础的NLP算法和模型，在职期间，项目.....，


# 简历相关知识点

## 基础知识
L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为‖w‖1。   
L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为‖w‖22。   
L1正则化可以使得参数稀疏化，即得到的参数是一个稀疏矩阵，可以用于特征选择。   
L2正则化可以防止模型过拟合

## bert
### 两个预训练任务的loss
1、上下文预测mask的字（80%，10%）  
2、NSP预测

### bert结构
input: token embedding + segment embedding + position embedding
transformer的encoder部分,每一层的encoder则是由multi-head-Attention + LN + feedforword + LN 
#### attention: 
解决问题：
GPU中并行加速计算   
长距离依赖   
QKV 矩阵， softmax，QK为权重，V加权
### 文本匹配中使用到的模型
在项目中，我们使用了有监督训练和无监督训练。前期业务方指定类目时，数据量较小，我们使用有监督训练。
#### 不训练
- BERT-whitening：https://spaces.ac.cn/archives/8069
直接用Bert不行，因为不是标准正交坐标系。  
均值变换为0、协方差矩阵变换为单位阵
#### 有监督
- simBERT（权重公开）
https://spaces.ac.cn/archives/7427   
有监督模型，结合UNILM生成任务和检索任务，实际应用中，两个我们使用时只保留了检索任务。    
数据集：一对相似文本，AB,BA同时放入batch中   
atteniton：sep之前双向，之后单向递归   
生成s2s任务:主要使用attention mask技术，对每一对文本，[sep]之后的文本，只能看见之前的token,之后的token会mask掉，让模型去递归预测   
检索：使用[CLS]token,（代表sep之前的句向量）在一个batch中mask掉自身，下一个文本是正样本，其他都是负样本
- 改进：RoFormer（开源）
https://spaces.ac.cn/archives/8454   
输入句子随机mask（BART)，多任务训练， 检索效果下降   
SimBERT的检索效果转移到RoFormer-Sim上去
### 自监督
# 任务目标
挑战：完成海量文本匹配任务 
- 数据量过亿，使用bert的nsp进行n x n匹配效果最好，但是不能实现一天内计算完成。
- 因此只能计算文本向量表示，最后统一进行矩阵计算和相似度排序。

# 对比学习的理解
在普通多分类模型中，计算交叉熵的时候，出现了特殊情况：**即类别过多（例如每条文本是一个类别，有过多文本）**，则使用对比学习。     
具体操作是：多分类转为二分类，对一个样本分别使用正样本采样和负样本采样，让模型去判断，这个采样的样本，是正样本还是负样本。
- 小样本
- 正样本好构造

# 模型：
BERT句向量不能直接用于计算cos相似度




# 面试记录
### 先导智能
工艺优化：运筹优化，调度优化，路线
缺陷检测：cv，曲线异常：焊接，采集电流波动，挖掘变量曲线模型，电机，毛刺
质量预测：预测算法，回归算法，产品质量预测，设备运行状况预测，部署传感器，寿命预测
知识图谱：关联分析：生产线，工艺段，因果推断，智能运维，
问答：对话机器人


# 字节(一面挂)
bert相关：
attention QKV, bert后续工作理解，bert的word piece




