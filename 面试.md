# 自我介绍
我比较熟悉NLP中的文本匹配，文本表征任务，以及基础的NLP算法和模型，在职期间，项目.....，


# 简历相关知识点
## bert
### 两个预训练任务的loss
1、上下文预测mask的字（80%，10%）  
2、NSP预测

### bert结构
input: token embedding + segment embedding + position embedding
transformer的encoder部分,每一层的encoder则是由multi-head-Attention + LN + feedforword + LN 
#### attention: 
解决问题：
GPU中并行加速计算   
长距离依赖   
QKV 矩阵， softmax，QK为权重，V加权
### 文本匹配中使用到的模型
在项目中，我们使用了有监督训练和无监督训练。前期业务方指定类目时，数据量较小，我们使用有监督训练。
#### 不训练
- BERT-whitening：https://spaces.ac.cn/archives/8069
直接用Bert不行，因为不是标准正交坐标系。  
均值变换为0、协方差矩阵变换为单位阵
#### 有监督
- simBERT（权重公开）
https://spaces.ac.cn/archives/7427   
有监督模型，结合UNILM生成任务和检索任务，实际应用中，两个我们使用时只保留了检索任务。    
数据集：一对相似文本，AB,BA同时放入batch中   
atteniton：sep之前双向，之后单向递归   
生成s2s任务:主要使用attention mask技术，对每一对文本，[sep]之后的文本，只能看见之前的token,之后的token会mask掉，让模型去递归预测   
检索：使用[CLS]token,（代表sep之前的句向量）在一个batch中mask掉自身，下一个文本是正样本，其他都是负样本
- 改进：RoFormer（开源）
https://spaces.ac.cn/archives/8454   
输入句子随机mask（BART)，多任务训练， 检索效果下降   
SimBERT的检索效果转移到RoFormer-Sim上去
### 自监督

